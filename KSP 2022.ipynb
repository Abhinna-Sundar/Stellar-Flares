{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy import units as u\n",
    "from astropy.io import fits\n",
    "from glob import glob\n",
    "from astropy.time import Time\n",
    "from astropy.timeseries import TimeSeries, aggregate_downsample\n",
    "from astropy.convolution import convolve, Box1DKernel\n",
    "import math\n",
    "from scipy.optimize import curve_fit as cf\n",
    "from scipy.special import erf\n",
    "import scipy.special\n",
    "import sympy as smp\n",
    "from scipy.integrate import quad\n",
    "from sympy import *\n",
    "from numpy.random import lognormal\n",
    "\n",
    "plt.style.reload_library()\n",
    "plt.style.use(['science','notebook','grid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Accessing all the .lc files , binning and returning as Count Rate vs Time of the Day (s)'''\n",
    "\n",
    "def access_files(filenames, bintime):  \n",
    "    hdul= fits.open(filename)\n",
    "    data=hdul[1].data\n",
    "    mjd         = data['time']*u.s\n",
    "    cts_per_sec = data['rate']\n",
    "    cts_error   = data['error']\n",
    "    if len(cts_per_sec)<=1:\n",
    "        pass\n",
    "    else:\n",
    "        dat = TimeSeries(time=Time(mjd, format=\"mjd\"))\n",
    "        dat.add_column(cts_per_sec, name=\"cts_per_sec\")\n",
    "        dat.add_column(cts_error, name=\"cts_error\")\n",
    "        dat_binned = aggregate_downsample(dat, time_bin_size=bintime*u.s)            \n",
    "        dat_binned_sec = (dat_binned.time_bin_start.value- dat_binned.time_bin_start.value[0])*86400\n",
    "    \n",
    "    return dat_binned, dat_binned_sec\n",
    "\n",
    "\n",
    "'''Smoothing the datavalues'''\n",
    "\n",
    "def smooth(data, kernelsize):\n",
    "    data = convolve(data, Box1DKernel(kernelsize))\n",
    "    return data\n",
    "\n",
    "'''Adjacent Averaging method for smoothening'''\n",
    "\n",
    "def adj_aver(arr):\n",
    "    for i in range(len(arr)):\n",
    "        if i ==0:\n",
    "            arr[i] = (arr[i+1] + arr[i])/2\n",
    "        elif i == len(arr)-1:\n",
    "            arr[i] = (arr[i-1] + arr[i])/2\n",
    "        else:\n",
    "            arr[i] = (arr[i-1] + arr[i] + arr[i+1])/3\n",
    "    return arr\n",
    "\n",
    "\n",
    "\n",
    "'''Checking the end time condition'''\n",
    "\n",
    "def check(lis, thresh):\n",
    "    for i in lis:\n",
    "        if i< thresh:\n",
    "            return True\n",
    "    return False \n",
    "\n",
    "\n",
    "'''End times extraction based on different conditions:\n",
    "Four steps must be implemented to find the end times of the flares. \n",
    "1. First, the flare end time should be less than that of the next flare start time; \n",
    "use this as the limiting condition, but in this case, it should not be the last flare as no next flare exists. \n",
    "2. Second, after we are in between the 2 flares, the times should also occur after the flare peaks. \n",
    "3. Now, you find the same level count rate as that of the start. \n",
    "4. If that does not exist, take the end toime to be \n",
    "the start time of the next flare.\n",
    "'''    \n",
    "    \n",
    "def end_times(dat_binned_sec, dat_binned, flares_start_times, flares_peaks_times):\n",
    "    decay=[]\n",
    "    decay_times=[]\n",
    "    end_decay=[]\n",
    "    end_decay_times=[]\n",
    "    \n",
    "    flares_end_times=[]\n",
    "    \n",
    "    dat_binned_counts = dat_binned['cts_per_sec'].tolist()\n",
    "    dat_binned_sec = dat_binned_sec.tolist()\n",
    "    \n",
    "    for j in range(0, len(flares_start_times)):\n",
    "        if j < len(flares_start_times)-1:\n",
    "            decay = dat_binned_counts[dat_binned_sec.index(flares_peaks_times[j]): dat_binned_sec.index(flares_start_times[j+1])]\n",
    "            decay_times = dat_binned_sec[dat_binned_sec.index(flares_peaks_times[j]): dat_binned_sec.index(flares_start_times[j+1])]\n",
    "            \n",
    "            if check(decay,dat_binned_counts[dat_binned_sec.index(flares_start_times[j])]) == True:      \n",
    "                for k in range(0, len(decay)):\n",
    "                    if decay[k]< dat_binned_counts[dat_binned_sec.index(flares_start_times[j])]:\n",
    "                        flares_end_times.append(decay_times[k-1])\n",
    "                        break\n",
    "            else:\n",
    "                flares_end_times.append(flares_start_times[j+1])\n",
    "        \n",
    "        else:\n",
    "            end_decay = dat_binned['cts_per_sec'][dat_binned_sec.index(flares_peaks_times[j]): len(dat_binned_sec)-1]\n",
    "            end_decay_times= dat_binned_sec[dat_binned_sec.index(flares_peaks_times[j]): len(dat_binned_sec)-1]\n",
    "            \n",
    "            if check(end_decay,dat_binned_counts[dat_binned_sec.index(flares_start_times[j])]) == True:\n",
    "                for k in range(0, len(end_decay)):\n",
    "                    if end_decay[k]< dat_binned_counts[dat_binned_sec.index(flares_start_times[j])]:\n",
    "                        flares_end_times.append(end_decay_times[k-1])\n",
    "                        break\n",
    "        \n",
    "            else:\n",
    "                flares_end_times.append(end_decay_times[-1])\n",
    "\n",
    "    return flares_end_times                    \n",
    "                                                                                    \n",
    "\n",
    "\n",
    "'''Detection of Flares, start and end times'''\n",
    "\n",
    "def peaks_and_start_times(dat_binned_sec, dat_binned, slope):\n",
    "    \n",
    "    flares_peaks_times=[]\n",
    "    flares_peaks_counts=[]\n",
    "    flares_start_times=[]\n",
    "    flares_index=[]\n",
    "\n",
    "    for i in range(len(dat_binned_sec)-3):\n",
    "            if (dat_binned['cts_per_sec'][i]< dat_binned['cts_per_sec'][i+1]< dat_binned['cts_per_sec'][i+2]<dat_binned['cts_per_sec'][i+3]) and (dat_binned['cts_per_sec'][i+3]>=slope*dat_binned['cts_per_sec'][i]):\n",
    "                for j in range(i+3,len(dat_binned_sec)-3): \n",
    "                    if (dat_binned['cts_per_sec'][j]>dat_binned['cts_per_sec'][j+1]>dat_binned['cts_per_sec'][j+2]>dat_binned['cts_per_sec'][j+3]):\n",
    "                        m = np.argmax(dat_binned['cts_per_sec'][i+3:j+1])\n",
    "                        \n",
    "                        ''' finding flare peaks, start and end times for the flares '''\n",
    "                        \n",
    "                        if dat_binned_sec[i+3+m] not in flares_peaks_times:\n",
    "                            flares_peaks_times.append((dat_binned_sec[i+3+m]))\n",
    "                            flares_peaks_counts.append(dat_binned['cts_per_sec'][i+3+m])\n",
    "                            flares_start_times.append(dat_binned_sec[i])\n",
    "                            #flares_index.append(list(np.arange(i, i+3+m)))\n",
    "                        break \n",
    "    flares_end_times = end_times(dat_binned_sec, dat_binned, flares_start_times, flares_peaks_times)\n",
    "    for i in range(len(flares_start_times)):\n",
    "        dat_binned_times_list = dat_binned_sec.tolist()\n",
    "        flares_index.append(list(np.arange(dat_binned_times_list.index(flares_start_times[i]), dat_binned_times_list.index(flares_end_times[i])+1, 1)))\n",
    "    #flares_index=list(flatten(flares_index))\n",
    "                      \n",
    "    return flares_peaks_times, flares_peaks_counts, flares_start_times, flares_end_times, flares_index\n",
    "\n",
    "\n",
    "\n",
    "'''Plotting the binned and smoothened datavalues, flare peaks (cleaned), start times, end times and background level'''\n",
    "\n",
    "def plotdata(filename,x,y,flares_peaks_times, flares_peaks_counts, starts, ends, bg_limit):\n",
    "    plt.figure(figsize=[15,6])\n",
    "    plt.title(filename[-21:-13])\n",
    "    plt.scatter(x,y,color='C0', s=7, label='data')\n",
    "    #plt.plot(x,y,color='r')\n",
    "    plt.scatter(flares_peaks_times,flares_peaks_counts, color='red', label='Flares peak')\n",
    "    #[plt.axvline(_x, linewidth=2, linestyle='--',color='magenta') for _x in starts]\n",
    "    #[plt.axvline(_x, linewidth=2, linestyle='--',color='black') for _x in ends]\n",
    "    #plt.axhline(bg_limit, linewidth=3, color='limegreen', label='Background Level')\n",
    "    plt.ylabel('Count Rate', fontsize=15)\n",
    "    plt.xlabel('Time of the Day (s)',fontsize=15)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "'''Individaul Flares plotting''' \n",
    "    \n",
    "def indi_flares(dat_binned_sec, dat_binned, flares_start_times, flares_end_times):\n",
    "    dat_binned['cts_per_sec'] = dat_binned['cts_per_sec'].tolist()\n",
    "    dat_binned_sec = dat_binned_sec.tolist()\n",
    "    for i in range(0, len(flares_start_times)):\n",
    "        plt.figure(figsize=[10,4])\n",
    "        x = dat_binned_sec[dat_binned_sec.index(flares_start_times[i]): dat_binned_sec.index(flares_end_times[i])+1]\n",
    "        y = dat_binned['cts_per_sec'][dat_binned_sec.index(flares_start_times[i]): dat_binned_sec.index(flares_end_times[i])+1]\n",
    "        plt.scatter(x,y,s=5)\n",
    "        \n",
    "    \n",
    "'''Background Extraction\n",
    "1. Background =  total -  flares (1st time) -------constant background\n",
    "2. Remove points whose peak points are above 3 sigma level above the background and get the mean background count rate.\n",
    "3. Repeat step 2 two-three times again until the bg level converges.\n",
    "4. Check which flares are remaining and return them.\n",
    "5. Get the final background level.\n",
    "\n",
    "Update the start and end times of the remaining flares'''\n",
    "\n",
    "def remove_flares(dat_binned_sec, dat_binned, flares_index):\n",
    "    index = list(set(flatten(flares_index)))\n",
    "    bg_time = np.delete(dat_binned_sec, index)\n",
    "    bg_cts = np.delete(dat_binned['cts_per_sec'], index)\n",
    "    bg_limit = np.mean(bg_cts)+ 3*np.std(bg_cts)\n",
    "    \n",
    "    return bg_time, bg_cts, bg_limit\n",
    "\n",
    "\n",
    "'''Choosing flares above backgound level'''\n",
    "\n",
    "def choose_flares_above_bg(flares_peaks_times,flares_peaks_counts,flares_start_times,flares_end_times,flares_index,bg_limit):\n",
    "    ind=[]\n",
    "    for i in range(len(flares_peaks_times)):\n",
    "        if flares_peaks_counts[i]<bg_limit:\n",
    "            ind.append(i)\n",
    "        else:\n",
    "            pass\n",
    "        flares_peaks_times_n = list(np.delete(flares_peaks_times, ind))\n",
    "        flares_peaks_counts_n = list(np.delete(flares_peaks_counts, ind))\n",
    "        flares_start_times_n = list(np.delete(flares_start_times, ind))\n",
    "        flares_end_times_n = list(np.delete(flares_end_times, ind))\n",
    "        flares_index_n = list(np.delete(flares_index, ind))\n",
    "                    \n",
    "    return flares_peaks_times_n,flares_peaks_counts_n,flares_start_times_n,flares_end_times_n,flares_index_n   \n",
    "\n",
    "\n",
    "'''Iterate # of times for cleaning background level'''\n",
    "\n",
    "def bg_extract(how_many_times, flares_peaks_times,flares_peaks_counts,flares_start_times,flares_end_times,flares_index):\n",
    "    for i in range(how_many_times):\n",
    "        bg_time, bg_cts, bg_limit = remove_flares(dat_binned_sec, dat_binned, flares_index)\n",
    "        print('bg level {} = '.format(i), bg_limit, ', bg mean {} = '.format(i), np.mean(bg_cts), ', bg std {} = '.format(i), np.std(bg_cts))\n",
    "        flares_peaks_times,flares_peaks_counts,flares_start_times,flares_end_times,flares_index = choose_flares_above_bg(flares_peaks_times,flares_peaks_counts,flares_start_times,flares_end_times,flares_index,bg_limit)\n",
    "        \n",
    "    return bg_time, bg_cts, bg_limit, flares_peaks_times,flares_peaks_counts,flares_start_times,flares_end_times,flares_index    \n",
    "\n",
    "\n",
    "'''Update the start and end times above background level'''\n",
    "\n",
    "def update_starts_ends(dat_binned_sec, dat_binned, flares_peaks_times, flares_peaks_counts, flares_start_times, flares_index, bg_limit, param_limit):\n",
    "    flares_cleaned_times=[]\n",
    "    flares_cleaned_counts=[]\n",
    "    flares_starts=[]\n",
    "    flares_ends=[]\n",
    "    flares_cleaned_peaks_times=[]\n",
    "    flares_cleaned_peaks_counts=[]\n",
    "    \n",
    "    for p in range(len(flares_start_times)):\n",
    "        counts = [dat_binned['cts_per_sec'][j] for j in flares_index[p]]\n",
    "        times = [dat_binned_sec[j] for j in flares_index[p]]\n",
    "        inds = [i for i,v in enumerate(counts) if v > bg_limit]\n",
    "        flares_counts = [counts[k] for k in inds]\n",
    "        flares_times = [times[k] for k in inds]\n",
    "        if len(flares_times)> param_limit:     # 10 parameter limit\n",
    "            flares_cleaned_counts.append(flares_counts)\n",
    "            flares_cleaned_times.append(flares_times) \n",
    "            flares_cleaned_peaks_times.append(flares_peaks_times[p])\n",
    "            flares_cleaned_peaks_counts.append(flares_peaks_counts[p])\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    for i in range(len(flares_cleaned_times)):\n",
    "        flares_starts.append(flares_cleaned_times[i][0])\n",
    "        flares_ends.append(flares_cleaned_times[i][-1])\n",
    "        \n",
    "    return flares_starts, flares_ends, flares_cleaned_peaks_times, flares_cleaned_peaks_counts, flares_cleaned_times, flares_cleaned_counts    \n",
    "\n",
    "\n",
    "'''Defining Convolution function for flares fitting'''\n",
    "\n",
    "def convolve_exp_norm(x, A, mu, sigma, alpha, bg):   ## alpha = lambda here\n",
    "    co = A * np.exp( alpha*mu+ (alpha**2) *(sigma**2)/2.0)\n",
    "    x_erf = (x - mu - alpha*sigma**2)/(np.sqrt(2.0)*sigma)\n",
    "    y = co * np.exp(-alpha*x) * (1.0 + scipy.special.erf(x_erf)) + bg\n",
    "    return y\n",
    "\n",
    "'''Lognormal function for flares'''\n",
    "\n",
    "def LogNormal(x, A, mu, sigma, bg):\n",
    "    return A/(x*sigma*np.sqrt(2*np.pi)) * np.exp(-(((np.log(x)-mu)**2)/(2*sigma**2))) + bg\n",
    "\n",
    "\n",
    "'''FRED fits'''\n",
    "\n",
    "def FRED(x, A, tau1, tau2, bg):\n",
    "    return A*(np.exp(2*np.sqrt(tau1/tau2)))*np.exp(-tau1/x-x/tau2) + bg\n",
    "\n",
    "\n",
    "'''Plotting the data with fitted convolution funtions on the detected flares'''\n",
    "\n",
    "def plot_final(filename, dat_binned, dat_binned_sec, flares_cleaned_peaks_times, flares_cleaned_peaks_counts, flares_cleaned_times, flares_cleaned_counts, flares_starts, flares_ends, bg_limit):\n",
    "    plt.figure(figsize=[15,6])\n",
    "    #plt.errorbar(np.array(dat_binned_sec), np.array(dat_binned['cts_per_sec']), yerr=np.array(dat_binned['cts_error']), fmt='.', color='C0', label='data') \n",
    "    plt.scatter(np.array(dat_binned_sec), np.array(dat_binned['cts_per_sec']), s=10, color='C0', label='data')\n",
    "    #plt.scatter(flares_cleaned_times,flares_cleaned_counts, s=20, color='g')\n",
    "    plt.scatter(flares_cleaned_peaks_times, flares_cleaned_peaks_counts, color='red', label='flare peaks')\n",
    "    for i in range(len(flares_cleaned_peaks_times)):\n",
    "        #p_opt, p_cov = cf(convolve_exp_norm, np.array(flares_cleaned_times[i]) , np.array(flares_cleaned_counts[i]),(flares_cleaned_peaks_counts[i],flares_cleaned_peaks_times[i], 1000, 0.001, bg_limit))\n",
    "        p_opt, p_cov = cf(LogNormal, np.array(flares_cleaned_times[i]) , np.array(flares_cleaned_counts[i]),(flares_cleaned_peaks_counts[i],flares_cleaned_peaks_times[i], 0.1, bg_limit))\n",
    "        #p_opt, p_cov = cf(FRED, np.array(flares_cleaned_times[i]) , np.array(flares_cleaned_counts[i]),(400, 3900500,120, bg_limit))\n",
    "        #plt.plot(np.array(flares_cleaned_times[i]), convolve_exp_norm(np.array(flares_cleaned_times[i]), *p_opt), color='red',linewidth=4, alpha=0.7,  label='Fit_GaussExp_convolve')\n",
    "        plt.plot(np.array(flares_cleaned_times[i]), LogNormal(np.array(flares_cleaned_times[i]), *p_opt), color='red',linewidth=4, alpha=0.7,  label='Fit_LogNormal')        \n",
    "        #plt.plot(np.array(flares_cleaned_times[i]), FRED(np.array(flares_cleaned_times[i]), *p_opt), color='red',linewidth=4, alpha=0.7,  label='Fit_FRED')        \n",
    "        plt.axvline(flares_starts[i], linewidth=2, linestyle='--',color='magenta')\n",
    "        plt.axvline(flares_ends[i], linewidth=2, linestyle='--',color='black')\n",
    "        plt.show()\n",
    "    plt.axhline(bg_limit, linewidth=3, color='limegreen', label='Background + 3 $\\sigma$')\n",
    "    plt.ylabel('Count rate')\n",
    "    plt.xlabel('Time of the day (s)')\n",
    "    plt.title(filename[31:39])\n",
    "    plt.legend()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhinna Sundar\\anaconda3\\lib\\site-packages\\erfa\\core.py:154: ErfaWarning: ERFA function \"utctai\" yielded 84563 of \"dubious year (Note 3)\"\n",
      "  warnings.warn('ERFA function \"{}\" yielded {}'.format(func_name, wmsg),\n",
      "C:\\Users\\Abhinna Sundar\\anaconda3\\lib\\site-packages\\erfa\\core.py:154: ErfaWarning: ERFA function \"utctai\" yielded 1 of \"dubious year (Note 3)\"\n",
      "  warnings.warn('ERFA function \"{}\" yielded {}'.format(func_name, wmsg),\n",
      "C:\\Users\\Abhinna Sundar\\anaconda3\\lib\\site-packages\\erfa\\core.py:154: ErfaWarning: ERFA function \"taiutc\" yielded 693 of \"dubious year (Note 4)\"\n",
      "  warnings.warn('ERFA function \"{}\" yielded {}'.format(func_name, wmsg),\n",
      "C:\\Users\\Abhinna Sundar\\anaconda3\\lib\\site-packages\\erfa\\core.py:154: ErfaWarning: ERFA function \"utctai\" yielded 692 of \"dubious year (Note 3)\"\n",
      "  warnings.warn('ERFA function \"{}\" yielded {}'.format(func_name, wmsg),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename =  20200311\n",
      "bg level 0 =  23.42281150817871 , bg mean 0 =  15.778782 , bg std 0 =  2.5480099\n",
      "bg level 1 =  23.98972201347351 , bg mean 1 =  16.317379 , bg std 1 =  2.5574477\n",
      "bg level 2 =  24.49691343307495 , bg mean 2 =  16.544498 , bg std 2 =  2.650805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhinna Sundar\\anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py:4454: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asarray(arr)\n",
      "C:\\Users\\Abhinna Sundar\\anaconda3\\lib\\site-packages\\scipy\\optimize\\minpack.py:828: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  warnings.warn('Covariance of the parameters could not be estimated',\n"
     ]
    }
   ],
   "source": [
    "path='E:/Krittika Internship/'\n",
    "filenames = sorted(glob(path+'*.lc'))\n",
    "\n",
    "for filename in filenames:  \n",
    "    bintime = 125 # binning time\n",
    "    kernelsize=8 # Kernel size for smoothening\n",
    "    slope=1.08 # what slope you require i.e. 4th point/1st point (Count rate)\n",
    "    how_many_times=3 # how many times you want to iterate for background estimation\n",
    "    param_limit= 15 # how many datapoints you set as limit for a flare (this is imp for fitting)\n",
    "    \n",
    "    dat_binned, dat_binned_sec = access_files(filenames, bintime)  # Accessing files and binning them (bintime in seconds)\n",
    "    print('filename = ',filename[31:39])\n",
    "    dat_binned['cts_per_sec'] = smooth(dat_binned['cts_per_sec'], kernelsize)  # Smoothening the datavalues using Boxcar\n",
    "    dat_binned['cts_error'] = smooth(dat_binned['cts_error'], kernelsize)\n",
    "    flares_peaks_times, flares_peaks_counts, flares_start_times, flares_end_times, flares_index = peaks_and_start_times(dat_binned_sec, dat_binned, slope)    \n",
    "    #plotdata(filename,dat_binned_sec,dat_binned['cts_per_sec'],flares_peaks_times, flares_peaks_counts, flares_start_times, flares_end_times, bg_limit)\n",
    "    bg_time, bg_cts, bg_limit, flares_peaks_times,flares_peaks_counts,flares_start_times,flares_end_times,flares_index = bg_extract(how_many_times, flares_peaks_times,flares_peaks_counts,flares_start_times,flares_end_times,flares_index)\n",
    "    flares_starts, flares_ends, flares_cleaned_peaks_times, flares_cleaned_peaks_counts, flares_cleaned_times, flares_cleaned_counts = update_starts_ends(dat_binned_sec, dat_binned, flares_peaks_times, flares_peaks_counts, flares_start_times, flares_index, bg_limit, param_limit)\n",
    "    plot_final(filename, dat_binned, dat_binned_sec, flares_cleaned_peaks_times, flares_cleaned_peaks_counts, flares_cleaned_times, flares_cleaned_counts, flares_starts, flares_ends, bg_limit)\n",
    "    #plt.savefig(str(filename)+'.pdf', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46249.99999999491"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SNR(time, counts, starttime, endtime, bintime):\n",
    "    S_B = counts[time.index(starttime):time.index(endtime)+1]  #Signal+bkg for flare region\n",
    "    B = counts[time.index(starttime)]  #assumed Background to be flare starttime countrate\n",
    "    S = [S_B - B for x in S_B] ## only signal\n",
    "    \n",
    "    return np.mean(S)*np.sqrt(bintime)/(np.sqrt(np.mean(S)+2*B))    #SNR = S/sqrt(S+2B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39.53678779418557"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SNR(list(dat_binned_sec), list(dat_binned['cts_per_sec']), flares_cleaned_times[5][0], flares_cleaned_times[5][-1], 125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import gridspec\n",
    "\n",
    "t = flares_cleaned_times[0]#[:100]\n",
    "a = flares_cleaned_counts[0]#[:100]\n",
    "\n",
    "#p_opt, p_cov = cf(FRED, np.array(t) , np.array(a),(flares_cleaned_peaks_counts[0], 3900500,120, bg_limit))     \n",
    "#p_opt, p_cov = cf(LogNormal, np.array(flares_cleaned_times[0]) , np.array(flares_cleaned_counts[0]),(flares_cleaned_peaks_counts[0],10, 0.1, bg_limit))\n",
    "p_opt, p_cov = cf(convolve_exp_norm, np.array(flares_cleaned_times[0]) , np.array(flares_cleaned_counts[0]),(flares_cleaned_peaks_counts[0],flares_cleaned_peaks_times[0], 1000, 0.001, bg_limit))\n",
    "chi=[]\n",
    "for i in range(len(t)):\n",
    "    if a[i] != 0:\n",
    "        chi.append((a[i]-convolve_exp_norm(np.array(t), *p_opt)[i])**2/(a[i]))\n",
    "    chisq = sum(chi)\n",
    "    contri= (a-convolve_exp_norm(np.array(t), *p_opt))**2/(a)\n",
    "    \n",
    "#plt.scatter(t,chi,s=5) \n",
    "\n",
    "fig = plt.figure(figsize=[10,8])\n",
    "spec = gridspec.GridSpec(ncols=1, nrows=2,hspace=0.5, height_ratios=[2, 1])\n",
    "ax0 = fig.add_subplot(spec[0])\n",
    "ax0.set_title('20200311')\n",
    "ax0.scatter(t, a,s=10)\n",
    "ax0.set_ylabel('Count Rate')\n",
    "#ax0.plot(np.array(t), FRED(np.array(t), *p_opt), color='red',linewidth=4, alpha=0.7,  label='Fit_FRED')\n",
    "#ax0.plot(np.array(t), LogNormal(np.array(t), *p_opt), color='red',linewidth=4, alpha=0.7,  label='Fit_LogNormal')\n",
    "ax0.plot(np.array(t), convolve_exp_norm(np.array(t), *p_opt), color='red',linewidth=4, alpha=0.7,  label='Fit_ExpGauss_convolve')\n",
    "ax0.legend()\n",
    "ax1 = fig.add_subplot(spec[1])\n",
    "ax1.scatter(t, chi,s=10, label='')\n",
    "ax1.set_xlabel('Time of the Day (s)')\n",
    "ax1.set_ylabel(r'$(data-model)^2/err^2$')\n",
    "ax1.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'Llamaradas-Estelares'...\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/lupitatovar/Llamaradas-Estelares.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhinna Sundar\\Downloads\\Llamaradas-Estelares\n"
     ]
    }
   ],
   "source": [
    "cd Llamaradas-Estelares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Flare_model import flare_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([21122.77679524,  1799.54904759,   426.21102135])"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_vals=[21122, 1800, 421] #[tpeak, FWHM, amp]\n",
    "popt, pcov = cf(flare_model, t,a, p0=init_vals,maxfev=10000)\n",
    "\n",
    "plt.scatter(np.array(t),np.array(a)-bg_limit*np.ones(len(t)),color='black', s=10)\n",
    "#plt.plot(np.array(t), flare_model(np.array(t), *init_vals),color='red', label='Initial Guess', alpha=0.3)\n",
    "plt.plot(np.array(t), flare_model(np.array(t), *popt),color='r', label='Model Fit', lw=2)\n",
    "plt.legend()\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Count Rate')\n",
    "\n",
    "popt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$A e^{2\\sqrt{\\tau_1/\\tau_2})} \\cdot e^{-\\tau_1/x-x/\\tau_2}$ + $bg$ where $\\sqrt{\\tau_1\\tau_2} = peak$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39.53678779418557"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(b)*np.sqrt(bintime)/(np.sqrt(np.mean(b)+2*a[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(dat_binned_sec, dat_binned['cts_per_sec'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Function\n",
    "\n",
    "$g(x) = A$ $exp(-(x-B)^2/C^2)$\n",
    "\n",
    "### Exponential Function\n",
    "\n",
    "$h(x) = exp (-Dx)$\n",
    "\n",
    "\n",
    "### Convolution Function\n",
    "\n",
    "$f(t)$ = $\\frac{1}{2} \\sqrt{\\pi}$ $A$ $C$ $exp\\left[D(B-t) + \\frac{C^2 D^2}{4}\\right]\\left[erf(Z) - erf(Z-\\frac{t}{C})\\right]$\n",
    "\n",
    "#### where $Z = \\frac{2B+ C^2 D}{2C}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\begin{cases} \\frac{A e^{- \\frac{B^{2}}{C^{2}}} e^{- \\frac{x^{2}}{C^{2}}} e^{D x} e^{\\frac{2 B x}{C^{2}}}}{D} - \\frac{A e^{- \\frac{B^{2}}{C^{2}}} e^{- \\frac{x^{2}}{C^{2}}} e^{- D \\left(t - x\\right)} e^{\\frac{2 B x}{C^{2}}}}{D} & \\text{for}\\: D e^{\\frac{B^{2}}{C^{2}}} e^{\\frac{x^{2}}{C^{2}}} \\neq 0 \\\\A t e^{- \\frac{B^{2}}{C^{2}}} e^{- \\frac{x^{2}}{C^{2}}} e^{\\frac{2 B x}{C^{2}}} & \\text{otherwise} \\end{cases}$"
      ],
      "text/plain": [
       "Piecewise((A*exp(-B**2/C**2)*exp(-x**2/C**2)*exp(D*x)*exp(2*B*x/C**2)/D - A*exp(-B**2/C**2)*exp(-x**2/C**2)*exp(-D*(t - x))*exp(2*B*x/C**2)/D, Ne(D*exp(B**2/C**2)*exp(x**2/C**2), 0)), (A*t*exp(-B**2/C**2)*exp(-x**2/C**2)*exp(2*B*x/C**2), True))"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f= A*smp.exp(-((x-B)**2)/(C**2))\n",
    "g= smp.exp(-(D*(x)))\n",
    "def convolve(f, g, x, lower_limit, upper_limit):\n",
    "    t = Symbol('t')\n",
    "    h = g.subs(x, t - x)\n",
    "    return integrate(f * h, (t, lower_limit, upper_limit))\n",
    "\n",
    "t= Symbol('t')\n",
    "convolve(f, g, x, 0, t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
